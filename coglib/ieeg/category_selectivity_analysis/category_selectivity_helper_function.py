"""
This script contains all helper functions to the category selectivity script
    authors: Alex Lepauvre
    alex.lepauvre@ae.mpg.de
    contributors: Katarina Bendtz, Simon Henin
    katarina.bendtz@tch.harvard.edu
    Simon.Henin@nyulangone.org
"""

import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from scipy.stats import ttest_ind, ranksums
from mne.stats.cluster_level import _pval_from_histogram

from general_helper_functions.data_general_utilities import (compute_dependent_variable,
                                                             load_epochs)


def compute_d_prime(data, labels, condition):
    """
    This function computes d primes for several conditions, according to the formula described here:
    https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0157109
    :param data: (1D numpy array) single trial observations
    :param labels: (1D numpy  array) label of each of the trial
    :param condition: (string) condition for which to compute the dprime. So say you have the different labels
    "face", "object", "letter" and "symbols", if you pass "face" as condition, the dprime will be computed for the face
    relative to all other categories.
    :return:
    d_prime: (float) dprime for the condition of interest
    """
    # Get the unique categories:
    categories = np.unique(labels)
    assert condition in categories, "The condition {} not found in the labels".format(condition)
    # Compute the mean of the condition of interest:
    mean_j = np.mean(data[np.where(labels == condition)])
    # Compute the variance of the condition of interest:
    var_j = np.std(data[np.where(labels == condition)]) ** 2
    # Compute the sum of mean of all other conditions:
    mean_i = np.mean([np.mean(data[np.where(labels == cate)]) for cate in categories if cate != condition])
    # Compute the sum of variance of all other conditions:
    var_i = np.mean([np.std(data[np.where(labels == cate)]) ** 2 for cate in categories if cate != condition])
    # Compute the dprime:
    d_prime = (mean_j - mean_i) / np.sqrt(0.5 * (var_j + var_i))

    return d_prime


def dprime_test(data_df, groups="channel", n_perm=1024, tail=0, p_val=0.05, n_jobs=1):
    """
    This function determines selectivity using the method described here:
    https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0157109
    Dprimes are computed for each category and compared to a null distribution generated by shuffling the labels n times
    :param data_df: (pandas data frame) contains the data to be tested. The dataframe should have a column called
    value containing the values to compare, a column called condition containing a string describing to which condition
    a given value belongs. Furthermore, the dataframe should have a column named
    whatever you like specified by the variable "groups" that contain a string describing to which group a set of values
    belong to which a separate test should be performed. Practically, if you want to test baseline vs onset period
    for  every electrode, the group can be "channels" to perform a test for each channel.
    :param groups: (string) column name from the data_df containing the group to which the test should be fitted
    separately.
    :param n_perm: (int) number of permutations to use
    :param tail: (-1, 0 or 1) tail of the test
    :param p_val: (float) pvalue threshold to consider something significant
    :param n_jobs: (int) how many jobs to use. Relevant as we have permutations
    :return:
    pandas data frame: contains the results of the dprime test
    """
    print("=" * 40)
    print("Welcome to dprime_test")
    # Table for results:
    results_df = pd.DataFrame()
    # Looping through each group:
    for group in data_df[groups].unique():
        print("Compute {} dprime_test".format(group))
        # Extract the data of this group:
        group_data = data_df.loc[data_df[groups] == group]
        # Extract the relevant info:
        data = group_data["value"].to_numpy()
        labels = group_data["condition"].to_numpy()
        # Get the unique categories:
        categories = np.unique(labels)
        # Compute the observed d primes:
        for cate in categories:
            obs_dprime = compute_d_prime(data, labels, cate)
            # Compute the null distribution, shuffling the labels n times:
            null = Parallel(n_jobs=n_jobs)(delayed(compute_d_prime)(
                data, labels[np.random.permutation(len(labels))], cate)
                                           for i in range(n_perm))
            # Append the observed value to the null to ground it:
            null.append(obs_dprime)
            # Generate the p value:
            pvalue = _pval_from_histogram(np.array([obs_dprime]), np.array(null), tail=tail)
            if pvalue < p_val:
                reject = True
            else:
                reject = False
            # Add to the results data frame:
            results_df = results_df.append(pd.DataFrame({
                "subject": group.split("-")[0],
                "channel": group,
                "metric": None,
                "reject": reject,
                "condition": cate,
                "stat": obs_dprime,
                "pval": pvalue,
                "effect_strength": obs_dprime
            }, index=[0]))

    return results_df


def highest_vs_second(data_df, groups="channel", test="wilcoxon", p_val=0.05):
    """
    This function compares activation of the highest condition vs the second highest condition.
    :param data_df: (pandas data frame) contains the data to be tested. The dataframe should have a column called
    value containing the values to compare, a column called condition containing a string describing to which condition
    a given value belongs. Furthermore, the dataframe should have a column named
    whatever you like specified by the variable "groups" that contain a string describing to which group a set of values
    belong to which a separate test should be performed. Practically, if you want to test baseline vs onset period
    for  every electrode, the group can be "channels" to perform a test for each channel.
    :param groups: (string) column name from the data_df containing the group to which the test should be fitted
    separately.
    :param test: (string) name of the test. Shouuld be either "t_test" or "wilcoxon", other tests are not supported as
    of now.
    :param p_val: (float) p value threshold to consider something significant.
    :return:
    pandas df: contains the results of the test
    """
    # Looping through each group:
    results_df = pd.DataFrame()
    for group in data_df[groups].unique():
        # Extract the data of this group:
        group_data = data_df.loc[data_df[groups] == group]
        # Find the highest condition (average):
        cond_avg = {}
        for cond in group_data["condition"].unique():
            cond_avg[cond] = np.mean(group_data.loc[group_data["condition"] == cond, "value"].to_list())
        # Find which is the max:
        max_cond = [cond for cond in cond_avg if cond_avg[cond] == max([cond_avg[key] for key in cond_avg])][0]
        # Get the values of the one condition:
        max_cond_val = group_data.loc[group_data["condition"] == max_cond, "value"].to_list()
        # Get the values of the second max condition other conditions:
        other_cond = [cond for cond in list(group_data["condition"].unique()) if cond != max_cond]
        second_cond = [cond for cond in other_cond if cond_avg[cond] == max([cond_avg[key] for key in other_cond])][0]
        second_cond_values = group_data.loc[group_data["condition"] == second_cond, "value"].to_list()
        # perform the test:
        if test == "wilcoxon":
            statistic, pvalue = ranksums(max_cond_val, second_cond_values, alternative="greater")
        elif test.lower() == "t-test" or test.lower() == "t_test" or test.lower() == "t test":
            statistic, pvalue = ttest_ind(max_cond_val, second_cond_values, alternative="greater")
        else:
            raise Exception("You have passed a test that is not supported!")

        # Now computing the d':
        if pvalue < p_val:
            reject = True
        else:
            reject = False
        # Compute the dprime:
        dprime = compute_d_prime(group_data["value"].to_numpy(), group_data["condition"].to_numpy(), max_cond)
        fav_cond = max_cond

        results_df = results_df.append(pd.DataFrame({
            "subject": group.split("-")[0],
            "channel": group,
            "metric": None,
            "reject": reject,
            "condition": fav_cond,
            "stat": statistic,
            "pval": pvalue,
            "effect_strength": dprime
        }, index=[0]))
    return results_df.reset_index(drop=True)


def highest_vs_all(data_df, groups="channel", test="wilcoxon", p_val=0.05):
    """
    This function compares activation to the highest vs all other conditions.
    :param data_df: (pandas data frame) contains the data to be tested. The dataframe should have a column called
    value containing the values to compare, a column called condition containing a string describing to which condition
    a given value belongs. Furthermore, the dataframe should have a column named
    whatever you like specified by the variable "groups" that contain a string describing to which group a set of values
    belong to which a separate test should be performed. Practically, if you want to test baseline vs onset period
    for  every electrode, the group can be "channels" to perform a test for each channel.
    :param groups: (string) column name from the data_df containing the group to which the test should be fitted
    separately.
    :param test: (string) name of the test. Shouuld be either "t_test" or "wilcoxon", other tests are not supported as
    of now.
    :param p_val: (float) p value threshold to consider something significant.
    :return:
    pandas df: contains the results of the test
    """
    # Looping through each group:
    results_df = pd.DataFrame()
    for group in data_df[groups].unique():
        # Extract the data of this group:
        group_data = data_df.loc[data_df[groups] == group]
        # Find the highest condition (average):
        cond_avg = {}
        for cond in group_data["condition"].unique():
            cond_avg[cond] = np.mean(group_data.loc[group_data["condition"] == cond, "value"].to_list())
        # Find which is the max:
        max_cond = [cond for cond in cond_avg if cond_avg[cond] == max([cond_avg[key] for key in cond_avg])][0]
        # Get the values of the one condition:
        max_cond_val = group_data.loc[group_data["condition"] == max_cond, "value"].to_list()
        # Get the values of all other conditions:
        other_cond = [cond for cond in list(group_data["condition"].unique()) if cond != max_cond]
        other_cond_values = group_data.loc[group_data["condition"].isin(other_cond), "value"].to_list()
        # perform the test:
        if test == "wilcoxon":
            statistic, pvalue = ranksums(max_cond_val, other_cond_values, alternative="greater")
        elif test.lower() == "t-test" or test.lower() == "t_test" or test.lower() == "t test":
            statistic, pvalue = ttest_ind(max_cond_val, other_cond_values, alternative="greater")
        else:
            raise Exception("You have passed a test that is not supported!")

        # Set whether we reject or not:
        if pvalue < p_val:
            reject = True
        else:
            reject = False
        # Compute the dprime:
        dprime = compute_d_prime(group_data["value"].to_numpy(), group_data["condition"].to_numpy(), max_cond)
        fav_cond = max_cond

        results_df = results_df.append(pd.DataFrame({
            "subject": group.split("-")[0],
            "channel": group,
            "metric": None,
            "reject": reject,
            "condition": fav_cond,
            "stat": statistic,
            "pval": pvalue,
            "effect_strength": dprime
        }, index=[0]))
    return results_df.reset_index(drop=True)


def prepare_test_data(root, signal, baseline_method, test_window, metric, cond_to_compare,
                      subject, baseline_time=(None, 0), crop_time=None, sel_conditions=None,
                      session="V1", task_name="Dur", preprocess_folder="epoching",
                      preprocess_steps="desbadcharej_notfil_autbadcharej_lapref",
                      channel_types=None, get_mni_coord=False, select_vis_resp=False,
                      vis_resp_folder=None, aseg=None, montage_space="T1", picks_roi=None,
                      multitaper_parameters=None, scal=1e0):
    """
    This function loads the epochs and format them according to the test passed
    :param root: (string or pathlib object) path to the bids root
    :param signal: (string) name of the signal to investigate
    :param baseline_method: (string) name of the method to compute the baseline correction, see baseline_rescale from
    mne for more details
    :param test_window: (list of two floats) contains the onset and offset of the test data
    :param metric: (string) name of the method to use to compute the data aggregation if wilcoxon or t_test is passed
    as a test
    :param cond_to_compare: (list of strings) list of conditions to compare.
    :param subject: (string) name of the subject
    :param baseline_time: (list of two floats) onset and offset for baseline correction
    :param crop_time: (list of two floats) time points to crop the epochs
    :param sel_conditions: (string) condition to select epochs from
    :param session: (string) name of the session
    :param task_name: (string) name of the task
    :param preprocess_folder: (string) name of the preprocessing folder
    :param preprocess_steps: (string) name of the preprocessing step to use
    :param channel_types: (dict or None) channel_type: True for the channel types to load
    :param get_mni_coord: (boolean) whether or not to get the mni coordinates for the channels
    :param select_vis_resp: (boolean) whether to use visual responsiveness as a filter for channels to load
    :param vis_resp_folder: boolean) the path to the visual responsiveness folder if visual responsiveness
    is to be used a filter for channels to load
    :param get_mni_coord: (boolean) whether or not to return the MNI coordinates!
    :param montage_space: (string) space of the electrodes localization, either T1 or MNI
    :param picks_roi: (list) list of ROI according to aseg to get the electrodes from. The epochs will be returned only
    with electrodes within this roi list
    :param multitaper_parameters: (dict) contains info to filtering steps on the data to get specific frequency bands
    {
        "freq_range": [8, 13],
        "step": 1,
        "n_cycle_denom": 2,
        "time_bandwidth": 4.0
    }
    :param aseg: (string) which segmentation to use. Relevant if you want to get channels only from a given ROI
    :param scal: (float) scale of the data if rescale needed
    :return:
    pandas df: contains data formated for the tests
    """
    print("=" * 40)
    print("Preparing sub-{} data".format(subject))
    if channel_types is None:
        channel_types = {"seeg": True, "ecog": True}
    # Load the data of the relevant subject with the right parameters (baseline correction...)
    epochs, mni_coord = load_epochs(root, signal, subject, session=session, task_name=task_name,
                                    preprocess_folder=preprocess_folder,
                                    preprocess_steps=preprocess_steps, channel_types=channel_types,
                                    condition=sel_conditions, baseline_method=baseline_method,
                                    baseline_time=baseline_time, crop_time=crop_time,
                                    select_vis_resp=select_vis_resp, vis_resp_folder=vis_resp_folder,
                                    aseg=aseg, montage_space=montage_space, get_mni_coord=get_mni_coord,
                                    picks_roi=picks_roi, filtering_parameters=multitaper_parameters)
    if epochs is None:
        return None, None, None
    # Format the data for the test:
    data_df = format_cat_sel_data(epochs, test_window, cond_to_compare, metric, scal)

    return data_df, epochs.info["sfreq"], mni_coord


def format_cat_sel_data(epochs, time_window, conditions, metric, scal):
    """
    This function computes the dependent variable for the category selectivity.
    :param epochs: (mne epochs object) contains the data to format
    :param time_window: (list of floats) time window over which to compute the metric
    :param conditions: (list of strings) list of conditions for which to compute the metric separately
    :param metric: (string) name of the metric to use. THere are several supported, check out
    general_helper_functions/data_general_utilities/compute_dependent_variable to find them
    :param scal: (float) scaling factor for the data (data are stored in volts by default, leading to very small
    values for things that are in microV...).
    :return:
    data_df: data frame containing the data
    """
    # Cropping the data according to the time windows:
    epochs_cropped = epochs.copy().crop(tmin=time_window[0],
                                        tmax=time_window[1])

    # Compute the dependent variable, i.e. for each selected trials in the cropped time window, computing some
    # measure of activation (i.e. mean, median...) so that we have one measure per trial:
    data_df = \
        compute_dependent_variable(epochs_cropped,
                                   conditions=conditions,
                                   metric=metric)
    # Scale the data:
    data_df["value"] = data_df["value"].apply(lambda x: x * scal)

    return data_df
